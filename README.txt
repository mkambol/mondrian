
This branch contains spike work done as a part of a performance spike
for BACKLOG-1443.  This spike explores the feasibility of pre-caching
cell data during native tuple load to avoid redundant fact table
queries.

To support this functionality, new logic was added to SqlTupleReader
to determine the set of measures currently in the query, add each to
the SELECT list generated for tuple retrieval, and gather the values
during tuple construction passes.  Then, prior to returning a tuple
list, determine whether it's possible to construct a new segment based
on the collected measure values, and if so, cache it.

Segment construction methods were added to SegmentBuilder to handle
creation of new segments given a tuple list and value list (see
SegmentBuilder.makeHeaderBodyPair).

I tested this implementation using several scenarios run with
Analyzer.  MDX generated by Analyzer is well suited to this approach,
because it in most cases will consolidate all attributes on the report
into a single [*NATIVE_CJ_SET].  For example, even if [Product] and
[Education Level] are on different axes, Analyzer will combine them in
a single NECJ and extract the components in subsequent expressions.

[*NATIVE_CJ_SET] AS
 'NONEMPTYCROSSJOIN(
[*BASE_MEMBERS__Product_],[*BASE_MEMBERS__Education Level_])'

This allows SqlTupleReader to effectively load all tuples from the
query using native eval, and with the changes in this spike cell data
can be loaded in a single swoop as well.

Some of the scenarios I tested included:
1)  one or more base measures on cols, single attribute on rows.
2)  one or more base measures on cols, cj'd attributes on cols.
3)  (2) + filter of specific members of a different dimension
4)  (2) + topcount  (handled as a Filter/Rank within Analyzer)
5)  CJ'd attributes on both rows and columns

In all cases the cell values were loaded during SqlTupleLoader
evaluation, and only a single fact table query was fired.

There are several things I did not complete with the spike, but I believe all
could be addressed in a full implementation.
1) Only base measures are handled.  A full implementation should walk
calculations to extract base measures, to make sure we cache all
measures that are likely to be requested.  
2) This spike does not handle context.  So any non-All members in
context are not factored into the constructed segment header/body.
3) Only DenseDoubleSegmentBody is created for segment bodies.  Need
logic to determine appropriate datatype and sparsity.
4) Virtual cubes are not handled.  
5) Agg tables are not handled.  
6) Constraints that cannot be handled (e.g. RolapNativeFilter) do not cause the
pre-cache logic to abort.  They should.
7) Tuple values below the top level are not handled.  So
[Product].[Brand] *should* result in a segment with the attribute
values of the brand's ancestors up the hierarchy, but currently this
is not implemented.

==Conclusion==

This is a risky change with potentially a lot of benefit.

The risks: it's a fairly complex change on top of the already brittle
native evaluation code.  Also, since we'd be constructing new segments
based on tuple queries, there's new risk for segments with wrong or
inconsistent data being cached.

The benefit: eliminate costly and redundant SQL queries during MDX
evaluation.  While the benefits may not be as great for some MDX
clients, Analyzer's "consolidate and extract" method of set creation
will maximize the number of cases where we take advantage of
pre-loading.

